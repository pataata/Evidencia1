{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Evidencia 1: Generación del set de datos** \n",
        "## Equipo:\n",
        "* Rubén Sánchez Mayén A01378379\n",
        "* Octavio Andrick Sánchez Perusquia A01378649\n",
        "* Joan Daniel Guerrero García A01378052"
      ],
      "metadata": {
        "id": "7cp7HuazWjju"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip evidencia1\n",
        "!unzip evidencia1/documentos-genuinos\n",
        "!unzip evidencia1/'documentos-con texto de otros'\n",
        "!unzip evidnecia1/documentos-sospechosos"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ensmth2UcqwA",
        "outputId": "cf91f6d2-26bd-4c92-8093-f3b36589f1df"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  documentos-sospechosos.zip\n",
            "  inflating: docmentos-sospechosos/FID-14.txt  \n",
            "  inflating: docmentos-sospechosos/FID-05.txt  \n",
            "  inflating: docmentos-sospechosos/FID-08.txt  \n",
            "  inflating: docmentos-sospechosos/FID-02.txt  \n",
            "  inflating: docmentos-sospechosos/FID-12.txt  \n",
            "  inflating: docmentos-sospechosos/FID-01.txt  \n",
            "  inflating: docmentos-sospechosos/FID-15.txt  \n",
            "  inflating: docmentos-sospechosos/FID-10.txt  \n",
            "  inflating: docmentos-sospechosos/FID-11.txt  \n",
            "  inflating: docmentos-sospechosos/FID-07.txt  \n",
            "  inflating: docmentos-sospechosos/FID-13.txt  \n",
            "  inflating: docmentos-sospechosos/FID-09.txt  \n",
            "  inflating: docmentos-sospechosos/FID-06.txt  \n",
            "  inflating: docmentos-sospechosos/FID-03.txt  \n",
            "  inflating: docmentos-sospechosos/FID-04.txt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Función para importar un archivo de texto\n",
        "def importar_texto(nombre_archivo):\n",
        "  with open(nombre_archivo) as archivo:\n",
        "      return archivo.read()"
      ],
      "metadata": {
        "id": "c1Wco9MZaaje"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Importar datos para entrenamiento y pruebas\n",
        "import os\n",
        "dir_genuinos = '/content/documentos-genuinos'\n",
        "dir_reutilizados = '/content/documentos-con texto de otros'\n",
        "dir_sospechosos = '/content/docmentos-sospechosos'\n",
        "\n",
        "def texto_de_archivos(directorio):\n",
        "  lista_textos = []\n",
        "  for subdir, dirs, files in os.walk(directorio):\n",
        "      for file in files:\n",
        "          lista_textos.append(importar_texto(f'{directorio}/{file}'))\n",
        "  return lista_textos\n",
        "\n",
        "#Colocar textos en listas\n",
        "textos_sospechosos = texto_de_archivos(dir_sospechosos)\n",
        "textos_reutilizados = []\n",
        "textos_genuinos = []\n",
        "#Separar textos reutilizados y genuinos de la carpeta de sospechosos\n",
        "for i in range(len(textos_sospechosos)):\n",
        "  if i % 2 == 0:\n",
        "    textos_reutilizados.append(textos_sospechosos[i])\n",
        "  else:\n",
        "    textos_genuinos.append(textos_sospechosos[i])\n",
        "\n",
        "#Cargar el resto de textos según su originalidad\n",
        "textos_reutilizados += texto_de_archivos(dir_reutilizados)\n",
        "textos_genuinos += texto_de_archivos(dir_genuinos)\n",
        "\n",
        "#Algunos ejemplos\n",
        "print(textos_genuinos[0])\n",
        "print(textos_reutilizados[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9vGf7YrZd7ZU",
        "outputId": "289e877f-1e3f-49a4-8997-363a87507dad"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Machine learning algorithms are often vulnerable to adversarial examples that have imperceptible alterations from the original counterparts but can fool the state-of-the-art models. It is helpful to evaluate or even improve the robustness of these models by exposing the maliciously crafted adversarial examples. In this paper, we present TextFooler, a simple but strong baseline to generate adversarial text. By applying it to two fundamental natural language tasks, text classification and textual entailment, we successfully attacked three target models, including the powerful pre-trained BERT, and the widely used convolutional and recurrent neural networks. We demonstrate three advantages of this framework: (1) effective—it outperforms previous attacks by success rate and perturbation rate, (2) utility-preserving—it preserves semantic content, grammaticality, and correct types classified by humans, and (3) efficient—it generates adversarial text with computational complexity linear to the text length.\n",
            "Test Collusion (TC) is a form of cheating in which test takers operate in groups to alter normal item responses. CT is becoming increasingly common, especially within large-scale, high-risk exams. However, research on CT detection methods remains sparse. This article proposes a new algorithm for the detection of CT, inspired by the selection of variables within the high-dimensional statistical analysis.\n",
            "This article presents a lemmatization system for the Urdu language, based on a novel dictionary lookup approach. The contributions made through this research are as follows: (1) the development of a large reference corpus for the Urdu language, (2) the exploration of the relationship between part-of-speech labels and the stemmer, and (3) ) the development of standard approaches for an Urdu stemmer. Additionally, we experiment with the impact of Part of Speech (PoS) in our proposed dictionary lookup approach. Empirical results showed that we achieved the best accuracy score of 76.44% through the proposed dictionary lookup approach.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Generar dataset de entrenamiento\n",
        "import distanciaCosenoTxts as disCos\n",
        "\n",
        "#Función que a partir de una base de datos, calcula la distancia coseno\n",
        "#para los enegramas especificados de cada texto en una lista objetivo, según la mayor \n",
        "#coincidencia en la base de datos\n",
        "def obtenerDistanciaDatos(base_datos, lista_objetivo, enegramas):\n",
        "  x = []\n",
        "  for i in range(len(lista_objetivo)):\n",
        "    x.append([0]*enegramas)\n",
        "    texto_objetivo = lista_objetivo[i]\n",
        "    for texto_base in base_datos:\n",
        "        for j in range(enegramas):\n",
        "          distancia = disCos.distanciaEntreTextos([texto_objetivo, texto_base], j+1)[0][1]\n",
        "          if distancia >= .8: #Detectar outliers y textos identicos\n",
        "            break\n",
        "          x[i][j] = max(distancia, x[i][j]) #La lista se encoje con cada outlier eliminado\n",
        "  return x"
      ],
      "metadata": {
        "id": "MuHcy68MkBGr"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ENEGRAMAS = 3\n",
        "\n",
        "#Agregar la distancia de todos los textos reutilizados al nuevo dataset\n",
        "xR = obtenerDistanciaDatos(textos_genuinos, textos_reutilizados, ENEGRAMAS)\n",
        "yR = [1]*len(xR)\n",
        "\n",
        "#Algunos ejemplos\n",
        "print(xR[:3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HeN9y2sKiXmf",
        "outputId": "d6365b5b-59ee-4371-950d-f4dde9f2e4e6"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.4949399496798402, 0.3910371502940005, 0.3376872344663547], [0.2602896031476768, 0.049160514400834666, 0.011629676092591607], [0.3268552425669442, 0.05852683750748628, 0.026462806201248152]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Agregar la distancia de n textos geuninos al dataset,\n",
        "#donde n es igual al numero de textos reutilizados\n",
        "xG = obtenerDistanciaDatos(textos_genuinos, textos_genuinos[:len(xR)], ENEGRAMAS)\n",
        "yG = [0]*len(xG)\n",
        "\n",
        "#Algunos ejemplos\n",
        "print(xG[:3])"
      ],
      "metadata": {
        "id": "4DlGGKywweqD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "757c4387-9c53-4129-bc10-7166f011defa"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.22631157994440146, 0.05300454699132827, 0.019534959790148375], [0.4174686989207958, 0.09658905768325902, 0.018163480488618104], [0.2775554665954842, 0.06804138174397717, 0.028782192785089703]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Unificar y serializar dataset\n",
        "x = xR + xG\n",
        "y = yR + yG\n",
        "with open(r'/content/dataset_reutilizacion_texto.csv', 'w') as archivo:\n",
        "  for i in range(len(x)):\n",
        "    for j in range(ENEGRAMAS):\n",
        "      archivo.write(\"%s,\" % (x[i][j]))\n",
        "    archivo.write(\"%s\\n\" % y[i])"
      ],
      "metadata": {
        "id": "V3iElA75pdkM"
      },
      "execution_count": 93,
      "outputs": []
    }
  ]
}